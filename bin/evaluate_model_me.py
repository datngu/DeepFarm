#!/usr/bin/env python

from keras.models import load_model, Model
from keras.layers import Dense, Dropout, Activation, Flatten, Conv1D, MaxPooling1D, Input, Lambda
from keras import metrics
from keras.callbacks import ModelCheckpoint, EarlyStopping
from sklearn.metrics import roc_auc_score, average_precision_score
import tensorflow as tf
import numpy as np
import pandas as pd
import argparse
import os, sys



parser = argparse.ArgumentParser(description = "Model evaluation...)", formatter_class=argparse.ArgumentDefaultsHelpFormatter)
parser.add_argument("--test", nargs='+', help = "testing data *.tfr generated by 'generate_tfr.py", required = True)
parser.add_argument("--model", help = "trained model (tensorflow, h5 format)", required = True)
parser.add_argument("--out", default = "model_evaluation", help = "model output files - only best model saved")
parser.add_argument('--batch_size', type=int, default = 1024, help = 'batch size for training')
parser.add_argument('--threads', type=int, default = 8, help = 'CPU cores for data pipeline loading')

args = parser.parse_args()

model_path = args.model
test_files = args.test
out = args.out
batch_size = args.batch_size
num_threads = args.threads


out_pkl = out + '.pkl'
out_csv = out + '.csv'

# Decoding function
def parse_record(record):
    name_to_features = {
        'seq': tf.io.FixedLenFeature([], tf.string),
        'label': tf.io.FixedLenFeature([], tf.string),
    }
    return tf.io.parse_single_example(record, name_to_features)



def decode_record(record):
    seq = tf.io.decode_raw(
        record['seq'], out_type=tf.float16, little_endian=True, fixed_length=None, name=None
    )
    label = tf.io.decode_raw(
        record['label'], out_type=tf.int8, little_endian=True, fixed_length=None, name=None
    )
    seq = tf.reshape(seq, [-1,4])
    #label = tf.cast(label, tf.float16)
    return (seq, label)



def get_dataset(record_file, num_threads = 8, batch = 512):
    dataset = tf.data.TFRecordDataset(record_file, num_parallel_reads = num_threads, compression_type = 'GZIP')
    dataset = dataset.map(parse_record, num_parallel_calls = num_threads)
    dataset = dataset.map(decode_record, num_parallel_calls = num_threads)
    dataset = dataset.shuffle(buffer_size = batch*10).batch(batch)
    return dataset
    

def get_evals(label, pred, n_out = 95):
    evals = {
        'auroc': np.zeros(n_out),
        'aupr': np.zeros(n_out),
    }
    for i in range(n_out):
        try:
            auroc = roc_auc_score(y_true=label[:, i], y_score=pred[:, i])
            aupr = average_precision_score(y_true=label[:, i], y_score=pred[:, i])
            evals['auroc'][i] = auroc
            evals['aupr'][i] =  aupr
        # only one class present
        except ValueError:
            evals['auroc'][i] = np.nan
            evals['aupr'][i] = np.nan
    return evals


model = load_model(model_file)

test_data = get_dataset(test_files, batch= batch_size, num_threads = num_threads)

iter_test_data = iter(test_data)

predictions = []
true_labels = []

for data, label in iter_test_data:
    preds = model.predict(data)
    predictions.append(preds)
    true_labels.append(label)

# Concatenate them along the first axis (vertically)
predictions = np.concatenate(predictions, axis=0)
true_labels = np.concatenate(true_labels, axis=0)

evals = get_evals(true_labels, predictions, n_ou = true_labels.shape[1])

## write results
try:
    import pickle
    with open(out_pkl, 'wb') as pickle_file:
        pickle.dump(evals, pickle_file)
    
    df = pd.DataFrame(evals)

    # Write to CSV without the index
    df.to_csv(out_csv, index=False)
except:
    pass
